from sentence_transformers import SentenceTransformer
from huggingface_hub import PyTorchModelHubMixin, HfApi

import numpy as np

import torch
import torch.nn as nn

import os
import sys

from model import AutoEncoder 
from config import MODEL_CATALOGUE # {model_name: model_hugging_face_id}



class CombinedSentenceTransformer(nn.Module, PyTorchModelHubMixin):
    def __init__(
        self, 
        models, 
        autoencoder_path: str = None,
        input_dim: int = 2048,  
        compressed_dim: int = 1024,
        device: str = "cuda",

        ## ? remove later
        repo_url: str = "your-repo-url",  
        pipeline_tag: str = "text-embedding",
        license: str = "mit",
        
    ):
        super().__init__()
        PyTorchModelHubMixin.__init__(self)

        self.models = models
        self.device = device
        for model in self.models:
            model.to(self.device)
            model.eval()
        
        if autoencoder_path:
            self.use_autoencoder = True
            self.input_dim       = INPUT_DIM
            self.compressed_dim  = COMPRESSED_DIM 
            self.autoencoder_path = autoencoder_path 
            self.autoencoder = AutoEncoder(input_dim=self.input_dim, compressed_dim=self.compressed_dim)
            print(f"fuckkk {self.input_dim}, {self.compressed_dim}")
            self.autoencoder.load_state_dict(torch.load(autoencoder_path, map_location=self.device))
            self.autoencoder.to(self.device)
            self.autoencoder.eval()  # Set to evaluation mode
        else:
            self.use_autoencoder = False
            self.autoencoder = None

    
    def generate_embeddings(self, embeddings_paths, output_path, batch_size=512):
        """
        Generate embeddings by loading precomputed embeddings, concatenating, applying autoencoder, and saving.

        Parameters:
        - embeddings_paths (List[str]): List of file paths to the embeddings generated by the individual models.
        - output_path (str): File path to save the generated embeddings.
        - batch_size (int): Batch size for processing.

        Returns:
        - None
        """

        embeddings_list = [np.load(path) for path in embeddings_paths]

        # Check that all embeddings have the same number of samples
        num_samples_list = [embeddings.shape[0] for embeddings in embeddings_list]
        if not all(num_samples == num_samples_list[0] for num_samples in num_samples_list):
            raise ValueError("All embeddings must have the same number of samples.")

        num_samples = num_samples_list[0]

        all_embeddings = []
        for i in range(0, num_samples, batch_size):
            batch_embeddings = [embeddings[i:i+batch_size] for embeddings in embeddings_list]

            concatenated_embeddings = np.concatenate(batch_embeddings, axis=1)
            concatenated_embeddings = torch.tensor(concatenated_embeddings, dtype=torch.float32).to(self.device)

            if self.use_autoencoder and self.autoencoder:
                with torch.no_grad():
                    compressed_embeddings = self.autoencoder.encoder(concatenated_embeddings)
                embeddings_to_save = compressed_embeddings.cpu().numpy()
            else:
                embeddings_to_save = concatenated_embeddings.cpu().numpy()

            all_embeddings.append(embeddings_to_save)

        final_embeddings = np.vstack(all_embeddings)
        np.save(output_path, final_embeddings)
    
    def forward(self, x):
        pass


    def encode(
        self,
        sentences,
        batch_size=512,
        show_progress_bar=False,
        device="cuda",
        convert_to_numpy=True,
        convert_to_tensor=False,
        normalize_embeddings=False,
        use_autoencoder: bool = None,
        **kwargs  # Capture all additional keyword arguments
    ):
        """
        Encode sentences by concatenating embeddings from all models.

        Parameters:
        - sentences (List[str]): List of sentences to encode. # I take pre-embedded sentences.
        - **kwargs: Additional keyword arguments. # this prevenets some errors, thanks o1

        Returns:
        - Combined embeddings as NumPy array or PyTorch tensor.
        """
        if device:
            self.to(device)

        # Filter out unexpected keyword arguments
        # Define allowed kwargs based on SentenceTransformer.encode's signature
        allowed_kwargs = {
            'convert_to_numpy',
            'convert_to_tensor',
            'normalize_embeddings',
            'output_value',  
			# Added based on SentenceTransformer.encode
            # Add other allowed kwargs if necessary
        }

        # Extract only allowed kwargs
        # Optionally, log or handle unexpected kwargs
        filtered_kwargs = {k: v for k, v in kwargs.items() if k in allowed_kwargs}
        unexpected_kwargs = set(kwargs.keys()) - allowed_kwargs
        if unexpected_kwargs:
            print(f"Warning: Ignoring unexpected keyword arguments: {unexpected_kwargs}")

        # Encode with each model
        embeddings = [
            model.encode(
                sentences,
                batch_size=batch_size,
                show_progress_bar=show_progress_bar,
                device=device,
                convert_to_numpy=True,  # Always get NumPy for concatenation
                normalize_embeddings=normalize_embeddings,
                **filtered_kwargs  # Pass only allowed kwargs
            )
            for model in self.models
        ]

        combined_embeddings = np.concatenate(embeddings, axis=1)
        combined_tensor = torch.tensor(combined_embeddings, dtype=torch.float32).to(device)

        apply_autoencoder = use_autoencoder if use_autoencoder is not None else self.use_autoencoder
        if apply_autoencoder and self.autoencoder:
            with torch.no_grad():
                compressed_embeddings = self.autoencoder.encoder(combined_tensor)
            embeddings_to_return = compressed_embeddings
        else:
            embeddings_to_return = combined_tensor 

        # Convert to desired format
        if convert_to_numpy:
            embeddings_to_return = embeddings_to_return.cpu().numpy()
        elif convert_to_tensor:
            embeddings_to_return = embeddings_to_return
        else:
            embeddings_to_return = embeddings_to_return

        return embeddings_to_return

    def save_pretrained(self, save_directory, **kwargs):
        os.makedirs(save_directory, exist_ok=True)
        torch.save(self.state_dict(), os.path.join(save_directory, "pytorch_model.bin"))
        
        for idx, model in enumerate(self.models):
            model_save_path = os.path.join(save_directory, f"sentence_transformer_{idx}")
            model.save(model_save_path)
        
        if self.use_autoencoder and self.autoencoder:
            torch.save(self.autoencoder.state_dict(), os.path.join(save_directory, "autoencoder.pth"))

    @classmethod
    def from_pretrained(cls, repo_id, **kwargs):
        repo_local_dir = Repository(local_dir=f"./{repo_id.split('/')[-1]}", clone_from=repo_id).local_dir
        state_dict = torch.load(os.path.join(repo_local_dir, "pytorch_model.bin"), map_location='cpu')
        model = cls(models=[], device='cpu')  # Initialize with empty models
        
        model.load_state_dict(state_dict)
        
        # Load SentenceTransformer models
        idx = 0
        model.models = []
        while True:
            model_path = os.path.join(repo_local_dir, f"sentence_transformer_{idx}")
            if os.path.exists(model_path):
                st_model = SentenceTransformer(model_path)
                model.models.append(st_model)
                idx += 1
            else:
                break
        
        # Load AutoEncoder if exists
        autoencoder_path = os.path.join(repo_local_dir, "autoencoder.pth")
        if os.path.exists(autoencoder_path):
            model.use_autoencoder = True
            model.autoencoder = AutoEncoder()
            model.autoencoder.load_state_dict(torch.load(autoencoder_path, map_location='cpu'))
        else:
            model.use_autoencoder = False
            model.autoencoder = None
        
        model.to(model.device)
        return model

    def mteb_eval(
        self, 
        task_name=None, 
        output_folder=None,
    ):
        import mteb ## i do late cuz of reasons.
        print("\n\n")
        if output_folder is None:
            output_folder = "results"

        if task_name is None:
            from mteb.benchmarks import MTEB_MAIN_EN
            tasks = MTEB_MAIN_EN
            print("Running evaluation on all MTEB tasks.")
        elif task_name == "Retrieval":
            print(f"Running evaluation on all Retrieval tasks")
            tasks = mteb.get_tasks(task_types=["Retrieval"]) 
            #tasks = mteb.get_tasks(tasks=[task_name])
        else:
            print(f"Running evaluation on the task: {task_name}")
            tasks = mteb.get_tasks(tasks=[task_name])

        evaluation = mteb.MTEB(
            tasks=tasks,
        )

        if self.use_autoencoder:
            print(f"Using concat + auto-encoder")
            output_folder = f"{output_folder}/{self.input_dim}_{self.compressed_dim}/X{CHECKPOINT_PATH}"
            results = evaluation.run(self, output_folder=output_folder)
        else:
            print("Attempting to evaluate without autoencoder.")
            output_folder = f"{output_folder}/no_autoencoder"
            results = evaluation.run(self, output_folder=output_folder)

        print(f"Evaluation results saved to {output_folder}")

        return results ## 
    
    def to(self, device):
        self.device = device
        for model in self.models:
            model.to(device)
        if self.use_autoencoder and self.autoencoder:
            self.autoencoder.to(device)


if __name__ == "__main__":
    
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    big   = ("e5", "mxbai")
    small = ("e5-small", "bge-small")

    nm1, nm2 = big 

    model_names = [MODEL_CATALOGUE[nm1], MODEL_CATALOGUE[nm2]]
    main_models = [SentenceTransformer(nm).to("cuda") for nm in model_names]

    if len(sys.argv) > 1 :
        INPUT_DIM       = int(sys.argv[1])
        COMPRESSED_DIM  = int(sys.argv[2])
        CHECKPOINT_PATH = sys.argv[3]
        path_to_checkpoint =  f"models_pth/{INPUT_DIM}_{COMPRESSED_DIM}/{CHECKPOINT_PATH}"

        combined_model = CombinedSentenceTransformer(
            models=main_models,  
            autoencoder_path=path_to_checkpoint,
            input_dim=INPUT_DIM,
            compressed_dim=COMPRESSED_DIM,
            device='cuda'
        )
        # combined_model.mteb_eval("NFCorpus")
        
#    e5_path_embeddings    =  "../data/new_e5_wiki_500k/train_embeddings.npy"
#    mxbai_path_embeddings =  "../data/new_mxbai_wiki_500k/train_embeddings.npy"
    
    data_1 =  np.load("../generate_data/embeddings_data/new_e5_wiki_500k/train_embeddings.npy")
    data_2 =  np.load("../generate_data/embeddings_data/new_mxbai_wiki_500k/train_embeddings.npy")

    data_3 =  np.load("../generate_data/embeddings_data/new_e5_wiki_500k/val_embeddings.npy")
    data_4 =  np.load("../generate_data/embeddings_data/new_mxbai_wiki_500k/val_embeddings.npy")

    data_train = np.concatenate((data_1, data_2), axis=1)
    data_val   = np.concatenate((data_3, data_4), axis=1)

    autoencoder = AutoEncoder(input_dim=INPUT_DIM, compressed_dim=COMPRESSED_DIM)
    autoencoder.load_state_dict(torch.load(autoencoder_path, map_location=device))

    autoencoder_train = autoencoder(data_train)[0]
    autoencoder_val   = autoencoder(data_train)[0]

    #np.save()
    #mse = nn.MSELoss() 
    #print(f"Loss is > ", mse(y, data))
    #emb_paths [e5_path_embeddings, mxbai_path_embeddings]













    
     
        

