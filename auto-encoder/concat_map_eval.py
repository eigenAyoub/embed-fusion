from sentence_transformers import SentenceTransformer
from huggingface_hub import PyTorchModelHubMixin, HfApi

import numpy as np

import torch
import torch.nn as nn

import sys
import os

from model import AutoEncoder 
from config import MODEL_CATALOGUE # {model_name: model_hugging_face_id}


class MappingNet(nn.Module):
    def __init__(self):
        super(MappingNet, self).__init__()

        self.layer_2048= nn.Sequential(
            nn.Linear(768, 2048),
            nn.BatchNorm1d(2048),
            nn.LeakyReLU(0.2, inplace=True)
        )

        self.layer_2048_768 = nn.Sequential(
            nn.Linear(2048, 768),
            nn.BatchNorm1d(768),
            nn.LeakyReLU(0.2, inplace=True)
        )
        
        self.layer_768_2048 = nn.Sequential(
            nn.Linear(768, 2048),
            nn.BatchNorm1d(2048),
            nn.LeakyReLU(0.2, inplace=True)
        )
        
#        self.final_layer = nn.Linear(1024, 768)
        
        self._initialize_weights()
                
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)
                
    def forward(self, x):
        out0 = self.layer_2048(x)                        # 1024 dim
        out1 = self.layer_2048_768(out0) + x       # 2048 dim
        out2 = self.layer_768_2048(out1)              # 1024 dim
#        final_out = self.final_layer(out2) + x             # 768 dim
        return out2, out0, out1


class CombinedSentenceTransformer(nn.Module, PyTorchModelHubMixin):
    def __init__(
        self, 
        models, 
        autoencoder_path: str = None,
        mapper_path: str = None,
        input_dim: int = 2048,  
        compressed_dim: int = 1024,
        device: str = "cuda",

        ## ? remove later
        repo_url: str = "your-repo-url",  
        pipeline_tag: str = "text-embedding",
        license: str = "mit",
        
    ):
        super().__init__()
        PyTorchModelHubMixin.__init__(self)
        ## ?  remove later

        self.models = models
        self.device = device
        for model in self.models:
            model.to(self.device)
            model.eval()
        
        if autoencoder_path:
            print(f"Using the following autoencoder after concat {mapper_path}")
            self.use_autoencoder = True
            self.autoencoder = AutoEncoder(input_dim=input_dim, compressed_dim=compressed_dim)
            self.autoencoder.load_state_dict(torch.load(autoencoder_path, map_location=self.device))
            self.autoencoder.to(self.device)
            self.autoencoder.eval()  # Set to evaluation mode
        else:
            self.use_autoencoder = False
            self.autoencoder = None

        if mapper_path:
            print(f"Using the following mapper after concat {mapper_path}")
            self.use_mapper= True
            self.mapper = MappingNet()
            self.mapper.load_state_dict(torch.load(mapper_path, map_location=self.device))
            self.mapper.to(self.device)
            self.mapper.eval()  # Set to evaluation mode
        else:
            self.use_mapper = False
            self.mapper = None

   
    
    def generate_embeddings(self, embeddings_paths, output_path, batch_size=512):
        """
        Generate embeddings by loading precomputed embeddings, concatenating, applying autoencoder, and saving.

        Parameters:
        - embeddings_paths (List[str]): List of file paths to the embeddings generated by the individual models.
        - output_path (str): File path to save the generated embeddings.
        - batch_size (int): Batch size for processing.

        Returns:
        - None
        """

        embeddings_list = [np.load(path) for path in embeddings_paths]

        # Check that all embeddings have the same number of samples
        num_samples_list = [embeddings.shape[0] for embeddings in embeddings_list]
        if not all(num_samples == num_samples_list[0] for num_samples in num_samples_list):
            raise ValueError("All embeddings must have the same number of samples.")

        num_samples = num_samples_list[0]

        all_embeddings = []
        for i in range(0, num_samples, batch_size):
            batch_embeddings = [embeddings[i:i+batch_size] for embeddings in embeddings_list]

            concatenated_embeddings = np.concatenate(batch_embeddings, axis=1)
            concatenated_embeddings = torch.tensor(concatenated_embeddings, dtype=torch.float32).to(self.device)

            if self.use_autoencoder and self.autoencoder:
                with torch.no_grad():
                    compressed_embeddings = self.autoencoder.encoder(concatenated_embeddings)
                embeddings_to_save = compressed_embeddings.cpu().numpy()
            else:
                embeddings_to_save = concatenated_embeddings.cpu().numpy()

            all_embeddings.append(embeddings_to_save)

        final_embeddings = np.vstack(all_embeddings)
        np.save(output_path, final_embeddings)
    
    def forward(self, x):
        pass


    def encode(
        self,
        sentences,
        batch_size=512,
        show_progress_bar=False,
        device="cuda",
        convert_to_numpy=True,
        convert_to_tensor=False,
        normalize_embeddings=False,
        use_autoencoder: bool = None,
        use_mapper: bool = None,
        **kwargs  # Capture all additional keyword arguments
    ):
        """
        Encode sentences by concatenating embeddings from all models.

        Parameters:
        - sentences (List[str]): List of sentences to encode. # I take pre-embedded sentences.
        - **kwargs: Additional keyword arguments. # this prevenets some errors, thanks o1

        Returns:
        - Combined embeddings as NumPy array or PyTorch tensor.
        """
        if device:
            self.to(device)

        # Filter out unexpected keyword arguments
        # Define allowed kwargs based on SentenceTransformer.encode's signature
        allowed_kwargs = {
            'convert_to_numpy',
            'convert_to_tensor',
            'normalize_embeddings',
            'output_value',  
			# Added based on SentenceTransformer.encode
            # Add other allowed kwargs if necessary
        }

        # Extract only allowed kwargs
        # Optionally, log or handle unexpected kwargs
        filtered_kwargs = {k: v for k, v in kwargs.items() if k in allowed_kwargs}
        unexpected_kwargs = set(kwargs.keys()) - allowed_kwargs
        if unexpected_kwargs:
            print(f"Warning: Ignoring unexpected keyword arguments: {unexpected_kwargs}")

        # Encode with each model
        embeddings = [
            model.encode(
                sentences,
                batch_size=batch_size,
                show_progress_bar=show_progress_bar,
                device=device,
                convert_to_numpy=True,  # Always get NumPy for concatenation
                normalize_embeddings=normalize_embeddings,
                **filtered_kwargs  # Pass only allowed kwargs
            )
            for model in self.models
        ]

        combined_embeddings = np.concatenate(embeddings, axis=1)
        combined_tensor = torch.tensor(combined_embeddings, dtype=torch.float32).to(device)

        apply_autoencoder = use_autoencoder if use_autoencoder is not None else self.use_autoencoder
        apply_mapper = use_mapper if use_mapper is not None else self.use_mapper

        if apply_autoencoder and self.autoencoder:
            with torch.no_grad():
                compressed_embeddings = self.autoencoder.encoder(combined_tensor)
            embeddings_to_return = compressed_embeddings
        elif apply_mapper and self.mapper:
            with torch.no_grad():
                compressed_embeddings = self.mapper(combined_tensor)
            embeddings_to_return = compressed_embeddings[2]
        else:
            embeddings_to_return = combined_tensor 

        # Convert to desired format
        if convert_to_numpy:
            embeddings_to_return = embeddings_to_return.cpu().numpy()
        elif convert_to_tensor:
            embeddings_to_return = embeddings_to_return
        else:
            embeddings_to_return = embeddings_to_return

        return embeddings_to_return

    def save_pretrained(self, save_directory, **kwargs):
        os.makedirs(save_directory, exist_ok=True)
        torch.save(self.state_dict(), os.path.join(save_directory, "pytorch_model.bin"))
        
        for idx, model in enumerate(self.models):
            model_save_path = os.path.join(save_directory, f"sentence_transformer_{idx}")
            model.save(model_save_path)
        
        if self.use_autoencoder and self.autoencoder:
            torch.save(self.autoencoder.state_dict(), os.path.join(save_directory, "autoencoder.pth"))

    @classmethod
    def from_pretrained(cls, repo_id, **kwargs):
        repo_local_dir = Repository(local_dir=f"./{repo_id.split('/')[-1]}", clone_from=repo_id).local_dir
        state_dict = torch.load(os.path.join(repo_local_dir, "pytorch_model.bin"), map_location='cpu')
        model = cls(models=[], device='cpu')  # Initialize with empty models
        
        model.load_state_dict(state_dict)
        
        # Load SentenceTransformer models
        idx = 0
        model.models = []
        while True:
            model_path = os.path.join(repo_local_dir, f"sentence_transformer_{idx}")
            if os.path.exists(model_path):
                st_model = SentenceTransformer(model_path)
                model.models.append(st_model)
                idx += 1
            else:
                break
        
        # Load AutoEncoder if exists
        autoencoder_path = os.path.join(repo_local_dir, "autoencoder.pth")
        if os.path.exists(autoencoder_path):
            model.use_autoencoder = True
            model.autoencoder = AutoEncoder()
            model.autoencoder.load_state_dict(torch.load(autoencoder_path, map_location='cpu'))
        else:
            model.use_autoencoder = False
            model.autoencoder = None
        
        model.to(model.device)
        return model

    def to(self, device):
        self.device = device
        for model in self.models:
            model.to(device)
        if self.use_autoencoder and self.autoencoder:
            self.autoencoder.to(device)



if __name__ == "__main__":

    big   = ("e5", "mxbai")
    small = ("e5-small", "bge-small")

    nm1, nm2 = small 

    model_names = [MODEL_CATALOGUE[nm1], MODEL_CATALOGUE[nm2]]
    main_models = [SentenceTransformer(nm).to("cuda") for nm in model_names]


    #CHECKPOINT_PATH = None 

    CHECKPOINT_PATH = sys.argv[1]

    combined_model = CombinedSentenceTransformer(
        models=main_models,  
        mapper_path ="best_model_jj.pth",
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )

    import mteb
    tasks = mteb.get_tasks(tasks=["NFCorpus"]) 
    evaluation = mteb.MTEB(tasks=tasks, eval_splits=["test"], metric="ndcg@10")
    results = evaluation.run(combined_model, output_folder = f"results/mix_shit12")
