from sentence_transformers import SentenceTransformer
from huggingface_hub import PyTorchModelHubMixin, HfApi

import numpy as np

import torch
import torch.nn as nn

import sys
import os

from model import AutoEncoder 
from config import MODEL_CATALOGUE # {model_name: model_hugging_face_id}

class MappingNet(nn.Module):
    def __init__(self):
        super(MappingNet, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(768, 1024),
            nn.ReLU(),
            nn.Dropout(p=0.3),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Dropout(p=0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(p=0.3),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Dropout(p=0.3),
            nn.Linear(512, 1024),
            nn.ReLU(),
            nn.Dropout(p=0.3),
            nn.Linear(1024, 768)
        )

    def forward(self, x):
        return self.layers(x)


big   = ("e5", "mxbai")
small = ("e5-small", "bge-small")

nm1, nm2 = small 
model_names = [MODEL_CATALOGUE[nm1], MODEL_CATALOGUE[nm2]]
main_models = [SentenceTransformer(nm).to("cuda") for nm in model_names]

INPUT_DIM       = int(sys.argv[1])
COMPRESSED_DIM  = int(sys.argv[2])

CHECKPOINT_PATH = None 

if len(sys.argv) > 3:
    CHECKPOINT_PATH = sys.argv[3]

import torch.nn as nn

class CombinedSentenceTransformer(nn.Module, PyTorchModelHubMixin):
    def __init__(
        self, 
        models, 
        autoencoder_path: str = None,
        mapper_path: str = None,
        input_dim: int = 2048,  
        compressed_dim: int = 1024,
        device: str = "cuda",

        ## ? remove later
        repo_url: str = "your-repo-url",  
        pipeline_tag: str = "text-embedding",
        license: str = "mit",
        
    ):
        super().__init__()
        PyTorchModelHubMixin.__init__(self)
        ## ?  remove later

        self.models = models
        self.device = device
        for model in self.models:
            model.to(self.device)
            model.eval()
        
        if autoencoder_path:
            print(f"Using the following autoencoder after concat {mapper_path}")
            self.use_autoencoder = True
            self.autoencoder = AutoEncoder(input_dim=input_dim, compressed_dim=compressed_dim)
            self.autoencoder.load_state_dict(torch.load(autoencoder_path, map_location=self.device))
            self.autoencoder.to(self.device)
            self.autoencoder.eval()  # Set to evaluation mode
        else:
            self.use_autoencoder = False
            self.autoencoder = None

        if mapper_path:
            print(f"Using the following mapper after concat {mapper_path}")
            self.use_mapper= True
            self.mapper = MappingNet()
            self.mapper.load_state_dict(torch.load(mapper_path, map_location=self.device))
            self.mapper.to(self.device)
            self.mapper.eval()  # Set to evaluation mode
        else:
            self.use_mapper = False
            self.mapper = None

   
    
    def generate_embeddings(self, embeddings_paths, output_path, batch_size=512):
        """
        Generate embeddings by loading precomputed embeddings, concatenating, applying autoencoder, and saving.

        Parameters:
        - embeddings_paths (List[str]): List of file paths to the embeddings generated by the individual models.
        - output_path (str): File path to save the generated embeddings.
        - batch_size (int): Batch size for processing.

        Returns:
        - None
        """

        embeddings_list = [np.load(path) for path in embeddings_paths]

        # Check that all embeddings have the same number of samples
        num_samples_list = [embeddings.shape[0] for embeddings in embeddings_list]
        if not all(num_samples == num_samples_list[0] for num_samples in num_samples_list):
            raise ValueError("All embeddings must have the same number of samples.")

        num_samples = num_samples_list[0]

        all_embeddings = []
        for i in range(0, num_samples, batch_size):
            batch_embeddings = [embeddings[i:i+batch_size] for embeddings in embeddings_list]

            concatenated_embeddings = np.concatenate(batch_embeddings, axis=1)
            concatenated_embeddings = torch.tensor(concatenated_embeddings, dtype=torch.float32).to(self.device)

            if self.use_autoencoder and self.autoencoder:
                with torch.no_grad():
                    compressed_embeddings = self.autoencoder.encoder(concatenated_embeddings)
                embeddings_to_save = compressed_embeddings.cpu().numpy()
            else:
                embeddings_to_save = concatenated_embeddings.cpu().numpy()

            all_embeddings.append(embeddings_to_save)

        final_embeddings = np.vstack(all_embeddings)
        np.save(output_path, final_embeddings)
    
    def forward(self, x):
        pass


    def encode(
        self,
        sentences,
        batch_size=512,
        show_progress_bar=False,
        device="cuda",
        convert_to_numpy=True,
        convert_to_tensor=False,
        normalize_embeddings=False,
        use_autoencoder: bool = None,
        use_mapper: bool = None,
        **kwargs  # Capture all additional keyword arguments
    ):
        """
        Encode sentences by concatenating embeddings from all models.

        Parameters:
        - sentences (List[str]): List of sentences to encode. # I take pre-embedded sentences.
        - **kwargs: Additional keyword arguments. # this prevenets some errors, thanks o1

        Returns:
        - Combined embeddings as NumPy array or PyTorch tensor.
        """
        if device:
            self.to(device)

        # Filter out unexpected keyword arguments
        # Define allowed kwargs based on SentenceTransformer.encode's signature
        allowed_kwargs = {
            'convert_to_numpy',
            'convert_to_tensor',
            'normalize_embeddings',
            'output_value',  
			# Added based on SentenceTransformer.encode
            # Add other allowed kwargs if necessary
        }

        # Extract only allowed kwargs
        # Optionally, log or handle unexpected kwargs
        filtered_kwargs = {k: v for k, v in kwargs.items() if k in allowed_kwargs}
        unexpected_kwargs = set(kwargs.keys()) - allowed_kwargs
        if unexpected_kwargs:
            print(f"Warning: Ignoring unexpected keyword arguments: {unexpected_kwargs}")

        # Encode with each model
        embeddings = [
            model.encode(
                sentences,
                batch_size=batch_size,
                show_progress_bar=show_progress_bar,
                device=device,
                convert_to_numpy=True,  # Always get NumPy for concatenation
                normalize_embeddings=normalize_embeddings,
                **filtered_kwargs  # Pass only allowed kwargs
            )
            for model in self.models
        ]

        combined_embeddings = np.concatenate(embeddings, axis=1)
        combined_tensor = torch.tensor(combined_embeddings, dtype=torch.float32).to(device)

        apply_autoencoder = use_autoencoder if use_autoencoder is not None else self.use_autoencoder
        apply_mapper = use_mapper if use_mapper is not None else self.use_mapper

        if apply_autoencoder and self.autoencoder:
            with torch.no_grad():
                compressed_embeddings = self.autoencoder.encoder(combined_tensor)
            embeddings_to_return = compressed_embeddings
        elif apply_mapper and self.mapper:
            with torch.no_grad():
                compressed_embeddings = self.mapper(combined_tensor)
            embeddings_to_return = compressed_embeddings
        else:
            embeddings_to_return = combined_tensor 

        # Convert to desired format
        if convert_to_numpy:
            embeddings_to_return = embeddings_to_return.cpu().numpy()
        elif convert_to_tensor:
            embeddings_to_return = embeddings_to_return
        else:
            embeddings_to_return = embeddings_to_return

        return embeddings_to_return

    def save_pretrained(self, save_directory, **kwargs):
        os.makedirs(save_directory, exist_ok=True)
        torch.save(self.state_dict(), os.path.join(save_directory, "pytorch_model.bin"))
        
        for idx, model in enumerate(self.models):
            model_save_path = os.path.join(save_directory, f"sentence_transformer_{idx}")
            model.save(model_save_path)
        
        if self.use_autoencoder and self.autoencoder:
            torch.save(self.autoencoder.state_dict(), os.path.join(save_directory, "autoencoder.pth"))

    @classmethod
    def from_pretrained(cls, repo_id, **kwargs):
        repo_local_dir = Repository(local_dir=f"./{repo_id.split('/')[-1]}", clone_from=repo_id).local_dir
        state_dict = torch.load(os.path.join(repo_local_dir, "pytorch_model.bin"), map_location='cpu')
        model = cls(models=[], device='cpu')  # Initialize with empty models
        
        model.load_state_dict(state_dict)
        
        # Load SentenceTransformer models
        idx = 0
        model.models = []
        while True:
            model_path = os.path.join(repo_local_dir, f"sentence_transformer_{idx}")
            if os.path.exists(model_path):
                st_model = SentenceTransformer(model_path)
                model.models.append(st_model)
                idx += 1
            else:
                break
        
        # Load AutoEncoder if exists
        autoencoder_path = os.path.join(repo_local_dir, "autoencoder.pth")
        if os.path.exists(autoencoder_path):
            model.use_autoencoder = True
            model.autoencoder = AutoEncoder()
            model.autoencoder.load_state_dict(torch.load(autoencoder_path, map_location='cpu'))
        else:
            model.use_autoencoder = False
            model.autoencoder = None
        
        model.to(model.device)
        return model

    def to(self, device):
        self.device = device
        for model in self.models:
            model.to(device)
        if self.use_autoencoder and self.autoencoder:
            self.autoencoder.to(device)


autoencoder_path_ = None
if CHECKPOINT_PATH is not None:
    autoencoder_path_ = f'models_pth/{INPUT_DIM}_{COMPRESSED_DIM}/{CHECKPOINT_PATH}'

"""
repo_url = "https://huggingface.co/benayad7/concat-e5-small-bge-small-01"

combined_model = CombinedSentenceTransformer(
    models=main_models,
    autoencoder_path=autoencoder_path_,  
    input_dim= INPUT_DIM,  
    compressed_dim= COMPRESSED_DIM,
    device='cuda' if torch.cuda.is_available() else 'cpu',
    #repo_url=repo_url,
    #pipeline_tag="text-embedding",
    #license="mit",
)


device = 'cuda' if torch.cuda.is_available() else 'cpu'
combined_model.to(device)



    # Repeat the process for the validation set
    e5_val_embeddings_path = "data/e5_wiki_500k/val_embeddings.npy"
    mxbai_val_embeddings_path = "data/mxbai_wiki_500k/val_embeddings.npy"
    output_val_embeddings_path = "data/combined_embeddings/val_embeddings.npy"

    combined_model.generate_embeddings(
        embeddings_paths=[e5_val_embeddings_path, mxbai_val_embeddings_path],
        output_path=output_val_embeddings_path,
        batch_size=512
    )

    print("Combined embeddings have been generated and saved.")

    e5_train_embeddings_path    = "../data/e5_wiki_500k/train_embeddings.npy"
    mxbai_train_embeddings_path = "../data/mxbai_wiki_500k/train_embeddings.npy"

    output_train_embeddings_path = "../data/mix_train_embeddings.npy"
    autoencoder_path = "models_pth/2048_768/013036.pth"

    e5_embeddings_sample    = np.load(e5_train_embeddings_path,    mmap_mode='r')
    mxbai_embeddings_sample = np.load(mxbai_train_embeddings_path, mmap_mode='r')

    e5_dim = e5_embeddings_sample.shape[1]
    mxbai_dim = mxbai_embeddings_sample.shape[1]

    INPUT_DIM = e5_dim + mxbai_dim
    COMPRESSED_DIM = 768 

"""

if __name__ == "__main__":


    combined_model = CombinedSentenceTransformer(
        models=main_models,  
        mapper_path ="best_model_01.pth",
#        input_dim=INPUT_DIM,
#        compressed_dim=COMPRESSED_DIM,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )

    # Generate combined embeddings for the training set
    #combined_model.generate_embeddings(
    #    embeddings_paths=[e5_train_embeddings_path, mxbai_train_embeddings_path],
    #    output_path=output_train_embeddings_path,
    #    batch_size=512
    #)

    import mteb
    tasks = mteb.get_tasks(tasks=["NFCorpus"]) 
    evaluation = mteb.MTEB(tasks=tasks, eval_splits=["test"], metric="ndcg@10")
    results = evaluation.run(combined_model, output_folder = f"results/mix_map")
